---
title: "train_on_multiple_videos"
author: "Niels van der Vegt"
date: "2023-12-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# SETTINGS
AGGREGATION_INTERVAL = 24  # seconds 
```

```{r, echo=F, message=F, results='hide'}
library(tools4watlas)
library(ggplot2)
library(ggspatial)
library(sf)
library(grid)
library(gridExtra)
library(sfheaders)
library(sp)
library(rgdal)
library(dplyr)
library(glue)
library(lubridate)
library(raster)
library(stringr) 
source('util.R')
devtools::install_local("../momentuHMM_2.0.0.tar.gz") 
```

```{r}
set.seed(12345)
```

```{r}
validation_data <- read.csv(
  glue('resources/validation_data_{AGGREGATION_INTERVAL}s.csv',
       AGGREGATION_INTERVAL=AGGREGATION_INTERVAL),
  sep=';'
)
validation_data$time <- as.POSIXct(validation_data$time, tz="Europe/Amsterdam")
```

# For each observed date, I will grab the whole day of WATLAS data
```{r}
training_data <- data.frame()
for (dunlin in unique(validation_data$tag)[1:3]) {
  watlas <- read.csv(glue('resources/watlas/dunlin-all/dunlin-{tag}.csv',
                          tag=dunlin))
  # convert watlas time to timezone from observation
  watlas$time <- ymd_hms(watlas$time, tz = "UTC")
  watlas$time <- with_tz(watlas$time, tzone = "Europe/Amsterdam")
  # apply same aggregation as validation data
  watlas <- atl_thin_data(
    watlas,
    interval=AGGREGATION_INTERVAL,
    method="aggregate"
  )
  # get properties
  watlas <- get_properties(watlas, x='X', y='Y')
  
  # get unique dates of validation data
  dunlin_validation_data <- validation_data[validation_data$tag == dunlin,]
  unique_dates <- unique(date(dunlin_validation_data$time))
  # add those dates to the training data
  for (unique_date in unique_dates) {
    day_interval <- interval(
      ymd_hms(paste(as.Date(unique_date), " 00:00:01"), tz="Europe/Amsterdam"), 
      ymd_hms(paste(as.Date(unique_date), " 23:59:59"), tz="Europe/Amsterdam"),
    )
    training_data <- bind_rows(
      training_data,
      watlas[watlas$time %within% day_interval,]
    )
  }
}
```

```{r}
# select and rename relevant columns
#rawData[which(rawData$step_length == 0), c("X", "Y", "speed_in")] <- NA
training_data[which(training_data$dtime > AGGREGATION_INTERVAL * 2),
              c( "speed_in", "step_length", "angle")] <- NA
training_data$angle <- training_data$angle * pi / 180  # convert angle to radians
```

```{r}
hmmData <- training_data[,c("tag", "time", "X", "Y", "speed_in")]
colnames(hmmData) <- c("ID", "time", "x","y", "speed_in")

hmmData <- momentuHMM::prepData(
  data=hmmData,
  type="UTM"
)

# overwrite the step and angle with our own data
hmmData$step <- training_data$step_length
hmmData$angle <- training_data$angle
```

# HMM parameters 
```{r}
nbStates=3

stateNames <- c(
  "inactive", 
  "active",
  "flying"
)

# distributions for all data strems
dist = list(
  step = "gamma", 
  angle = "wrpcauchy",
  speed_in = "gamma"
)

# priors / initial distribution settings 
stepPar0<-c(
  1.5, 6, 140,  #  mean inactive, mean active, mean flying
  1, 3, 224,  #  sd inactive, sd active...
  0.17, 6.100306e-08, 9.116700e-08  #  zero mass inactive, zero mass active...
)
anglePar0 = c(
  0.3, 0.6, 0.8  #  concentration inactive, concentration active...
)
speedPar0 <- c(
  0.05, 1.776849e-01, 2.408613,
  0.03, 1.833827e-02, 3.110865,
  0.1866896, 6.947236e-08, 9.116700e-08
)
```

```{r}
# Set step design matrix such that
# mu3 > mu2 > mu1
stepDM <- matrix(
  c(1,0,0,0,0,0,0,0,0,
    1,1,0,0,0,0,0,0,0,
    1,1,1,0,0,0,0,0,0,
    0,0,0,1,0,0,0,0,0,
    0,0,0,0,1,0,0,0,0,
    0,0,0,0,0,1,0,0,0,
    0,0,0,0,0,0,1,0,0,
    0,0,0,0,0,0,0,1,0,
    0,0,0,0,0,0,0,0,1),
  nrow = 3*nbStates,
  byrow = T,
  dimnames = list(c(paste0("mu_", 1:nbStates),
                    paste0("sd_", 1:nbStates),
                    paste0("zeromass_", 1:nbStates)),
                  c("wmu_1","wmu_2","wmu_3",
                    paste0("wsd_", 1:nbStates),
                    paste0("wzeromass_", 1:nbStates))))
stepworkBounds <- matrix(
  c(-Inf,Inf,
    0,Inf,
    0,Inf,
    -Inf,Inf,
    -Inf,Inf,
    -Inf,Inf,
    -Inf,Inf,
    -Inf,Inf,
    -Inf,Inf),
  nrow=ncol(stepDM),
  byrow = TRUE,
  dimnames=list(c("wmu_1","wmu_2","wmu_3",
                  paste0("wsd_", 1:nbStates),
                  paste0("wzeromass_", 1:nbStates)),
                c("lower","upper")))

Par0<- momentuHMM::getParDM(
  data = hmmData,
  nbStates = nbStates,
  zeroInflation = list(step = T, speed_in = T),
  dist = list(step = 'gamma', speed_in = 'gamma'),
  Par = list(step = stepPar0, speed_in = speedPar0),
  DM = list(step = stepDM, speed_in = stepDM),
  workBounds = list(step = stepworkBounds, speed_in = stepworkBounds)
)

# Force flight zero mass to be next to none, we really don't
# expect any of the flight localisations to have a step length
# of 0 or a speed of 0
fixPar = list(
  step = c(NA, NA, NA,
           NA, NA, NA,
           NA, NA, stats::qlogis(1.e-100)),
  speed_in = c(NA, NA, NA,
               NA, NA, NA,
               NA, NA, stats::qlogis(1.e-100))
)
```


```{r, echo=F, message=F, results='hide'}
m1 <- momentuHMM::fitCTHMM(
  data = hmmData, 
  stateNames = stateNames,
  nbStates = nbStates, 
  dist = dist, 
  Par0 = list(
    step=Par0$step, 
    angle=anglePar0,
    speed_in=Par0$speed_in
  ),
  DM=list(
    step=stepDM,
    speed_in=stepDM
  ),
  workBounds = list(
    step=stepworkBounds,
    speed_in=stepworkBounds
  ),
  fixPar=fixPar,
  #nlmPar=list(print.level=2),
  formula=~ID+0,
  formulaDelta=~ID+0
)
```

```{r}
# decode most likely state sequence
states <- momentuHMM::viterbi(m1)
# derive percentage of time spent in each state
table(states)/nrow(hmmData)
# plot results for model m2
plot(m1)
```

```{r}
training_data$pred_state <- states
training_data[training_data$pred_state == 1,]$pred_state <- 'inactive'
training_data[training_data$pred_state == 2,]$pred_state <- 'active'
training_data[training_data$pred_state == 3,]$pred_state <- 'flying'
training_data$pred_state <- as.factor(training_data$pred_state)
```


# plot with fragmented sections (after 2x aggregation interval new section)
```{r}
get_linestrings <- function(localisations) {
  # start the segment with the first localisation
  segment <- matrix(as.numeric(localisations[1, c("X", "Y")]), nrow=1, ncol=2)
  behaviours <- c()  # one behaviour for each linestring
  linestrings <- list()  # collection of linestrings
  
  # add new linestring to collection of linestrings, and save the behaviour
  # this linestring should have
  add_segment <- function(i) {
    # if there is just 1 localisation we can't create a line, so skip it
    if (nrow(segment) > 1) {
      linestring <- st_linestring(x = segment, dim = "XY")
      linestrings[[length(linestrings) + 1]] <<- linestring
      behaviours <<- c(behaviours, as.character(localisations[i - 1, "pred_state"])) 
    } 
  }
  
  for(i in 1:nrow(localisations)) {
    if (i == 1) next  # already added during initialisation
    # if 
    if (
      localisations[i, "tag"] != localisations[i - 1, "tag"] ||
      localisations[i, "dtime"] > AGGREGATION_INTERVAL * 2
    ) {
      add_segment(i)
      segment <- matrix(
        as.numeric(localisations[i, c("X", "Y")]), 
        nrow=1, ncol=2
      )  # create new linestring
      next
    }
    # else, we're going to add to an existing linestring
    if (localisations[i, "pred_state"] != localisations[i - 1, "pred_state"]) {
      add_segment(i)
      # as this is an extension of the previous linestring we add the previous point
      segment <- matrix(
        as.numeric(
          c(localisations[i - 1, c("X", "Y")], 
            localisations[i, c("X", "Y")])
        ),
        nrow=2, ncol=2, byrow=T
      )
      next
    }
    # same behaviour, just extend the linestring
    segment <- rbind(segment, matrix(as.numeric(localisations[i, c("X", "Y")]), nrow=1, ncol=2))
    # add last segment if not already added
    if (i == nrow(localisations)) add_segment(i)
  }
  
  linestrings <- st_sfc(linestrings, crs=32631)
  linestrings <- st_as_sf(linestrings)
  linestrings$pred_state <- behaviours
  return(linestrings)
}
```

```{r}
plot_data <- training_data[training_data$tag == 2960,]
feat_linestrings <- get_linestrings(plot_data)
feat_points <- st_as_sf(plot_data, coords=c('X','Y'), crs=st_crs(32631))

# WGS 4326 in order to use satallite / map tiles
feat_linestrings <- st_transform(feat_linestrings, crs=4326)
feat_points <- st_transform(feat_points, crs=4326)

# TODO mapbox logo
ggplot() +
  annotation_map_tile("cartodark", zoom=12) +
  layer_spatial(feat_points, aes(col=as.factor(pred_state)), alpha=.5) +
  layer_spatial(feat_linestrings, aes(col=as.factor(pred_state)), alpha=.5)
```

# plot with behaviour in relation to griend
```{r}
# Libraries
library(tidyverse)
library(viridis)
library(hrbrthemes)
library(mapdata)

feat_active <- st_transform(st_as_sf(feat_active, coords=c('X', 'Y'), crs=32631), crs=4326)

feat_inactive <- training_data[training_data$pred_state == 1,]
feat_active <- training_data[training_data$pred_state == 2,]
feat_flying <- training_data[training_data$pred_state == 3,]
# xlim / ylim based on bounding box of training data
# raster, if any values 0 empty


# plot
ggplot(feat_active, aes(x=X, y=Y)) + 
  geom_bin_2d(bins=30) + 
  annotation_map_tile("cartodark", zoom=12) 
) 
```

# Validation Plots
```{r}
# Get the bbox of the total study area
feat_validation_data <- st_as_sf(validation_data, coords=c("X", "Y"), crs=st_crs(32631))
feat_validation_data <- st_transform(feat_validation_data, crs=4326)
feat_study_area_bbox <- st_as_sfc(st_bbox(feat_validation_data))
```

```{r}
theme_behaviour_plot <- theme(
  axis.title.x=element_blank(),
  axis.text.x=element_blank(),
  axis.ticks.x=element_blank(),
  axis.title.y=element_blank(),
  axis.text.y=element_blank(),
  axis.ticks.y=element_blank()
)

get_location_plot <- function(
    training_data
    ) {
  feat_training_data <- st_as_sf(training_data, coords=c("X", "Y"), crs=st_crs(32631))
  feat_training_data <- st_transform(feat_training_data, crs=4326)
  feat_bbox <- st_as_sfc(st_bbox(feat_training_data))
  if(nrow(training_data) > 1) {
    feat_bbox <- st_buffer(feat_bbox, dist=100, endCapStyle='SQUARE', joinStyle='SQUARE')
  }
  
  return(ggplot() +
           annotation_map_tile(
             type="osm", 
             zoom=14,
             #progress='none'
            ) +
           layer_spatial(feat_study_area_bbox, fill=NA) + 
           layer_spatial(feat_bbox, fill='red', alpha=0.7)
        )
}
  
get_behaviour_track_plot <- function(
    name,
    feat_points,
    feat_lines = NULL
  ) {
  
  cols <- c(
    "inactive" = "red", 
    "active" = "blue", 
    "flying" = "darkgreen",
    "mixed" = "grey"
  )
  
  track_plot <- ggplot() +
    layer_spatial(feat_points, aes(col=plot_color), alpha=0.5) +
    scale_colour_manual(
      name = "Predicted State",
      values = cols,
      drop=F
    ) +
    theme_behaviour_plot
    
  
  if(!is.null(feat_lines) && nrow(feat_lines) > 0) {
    track_plot <- track_plot + layer_spatial(
      feat_lines, 
      aes(col=plot_color),
      alpha=0.5)
  }
  
  return(track_plot)
}

# get observation segments
for (observation_path in unique(validation_data$observation_path)[1:3]) {
  # some preprocessing for ggplot2
  observation <- validation_data[validation_data$observation_path == observation_path,]
  observation$behaviour <- as.factor(observation$behaviour)
  
  observation_interval <- interval(min(observation$time), max(observation$time))
  localisations <- training_data[training_data$tag == observation$tag[1],]
  localisations <- training_data[training_data$time %within% observation_interval,]
  # TODO this really shouldn't happen if we use all the training data
  if (nrow(localisations) == 0) next
  
  observation$plot_color <- observation$pure_behaviour
  localisations$plot_color <- localisations$pred_state
  
  feat_observation <- st_as_sf(observation, coords=c('X','Y'), crs=32631)
  feat_prediction <- st_as_sf(localisations, coords=c('X','Y'), crs=32631)
  feat_prediction_linestrings <- get_linestrings(localisations)
  
  feat_observation$plot_color <- observation$pure_behaviour
  feat_prediction$plot_color <- localisations$pred_state
  feat_prediction_linestrings$plot_color <- as.factor(feat_prediction_linestrings$pred_state)
  
  plot_location <- get_location_plot(localisations)
  plot_observation <- get_behaviour_track_plot(
    "Observed State",
    feat_points = feat_observation
  )
  plot_prediction <- get_behaviour_track_plot(
    "Predicted State",
    feat_points = feat_prediction,
    feat_lines = feat_prediction_linestrings
  )

  grid.arrange(
    plot_location,
    plot_observation, 
    plot_prediction
  )
}
```

# Validation Analytics
```{r}
validation_stats <- data.frame()

# validating the validation: check the following
# n predicted localisations should be the same as n observed localisations
# time of localisations should be the same as time of observed localisations

get_percent_behaviour <- function(data, column, behaviour) {
  return(nrow(data[data[,column] == behaviour,]) / nrow(data))
}

# for each behaviour - calculate true positive + false positive as well as percentages
for (observation_path in unique(validation_data$observation_path)[1:3]) {
  # some preprocessing for ggplot2
  observation <- validation_data[validation_data$observation_path == observation_path,]
  observation$behaviour <- as.factor(observation$behaviour)
  
  observation_interval <- interval(min(observation$time), max(observation$time))
  localisations <- training_data[training_data$tag == observation$tag[1],]
  localisations <- training_data[training_data$time %within% observation_interval,]
  # TODO this really shouldn't happen if we use all the training data
  if (nrow(localisations) == 0) next
  if (nrow(localisations) != nrow(observation)) 
    print("Different amount of predictions and observations")
  
  obs_percent_active <- get_percent_behaviour(observation, 'pure_behaviour', 'active')
  obs_percent_inactive <- get_percent_behaviour(observation, 'pure_behaviour', 'inactive')
  obs_percent_flying <- get_percent_behaviour(observation, 'pure_behaviour', 'flying')
  
  pred_percent_active <- get_percent_behaviour(localisations, 'pred_state', 'active')
  pred_percent_inactive <- get_percent_behaviour(localisations, 'pred_state', 'inactive')
  pred_percent_flying <- get_percent_behaviour(localisations, 'pred_state', 'flying')
    
  validation_stats <- bind_rows(
      validation_stats,
      data.frame(
        id=observation_path,
        obs_percent_active=obs_percent_active,
        obs_percent_inactive=obs_percent_inactive,
        obs_percent_flying=obs_percent_flying,
        pred_percent_active=pred_percent_active,
        pred_percent_inactive=pred_percent_inactive,
        pred_percent_flying=pred_percent_flying
      )
    )
}
```

```{r}
m_active <- lm(obs_percent_active ~ pred_percent_active, data=validation_stats)
plot(obs_percent_active ~ pred_percent_active, data=validation_stats)
abline(m_active)
```





